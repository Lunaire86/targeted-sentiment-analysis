{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[(Source)](https://medium.com/modern-nlp/productionizing-nlp-models-9a2b8a0c7d14)\n",
    "\n",
    "## Cost optimisations\n",
    "### Caching\n",
    "\n",
    "Unlike word2vec and glove which are fixed vocab non-contextual embeddings,\n",
    "language models like ELMo and BERT are contextual and do not have any\n",
    "fixed vocabulary. The downside of this is that the word embedding needs\n",
    "to be calculated every time through the model. This became quite a trouble\n",
    "for us as we saw heavy CPU spikes due to model processing.\n",
    "\n",
    "Since our text phrases had an average length of 5 and were repetitive in\n",
    "nature, we cached embeddings of the phrase to avoid re-computations.\n",
    "By just adding this small method to our code we got a 20x speedup üèÑ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```\n",
    "#Earlier\n",
    "language_model.get_sentence_embedding(sentence)\n",
    "\n",
    "#Later\n",
    "from cachetools import LRUCache, cached\n",
    "\n",
    "@cached(cache=LRUCache(maxsize=10000))\n",
    "def get_sentence_embedding(sentence):\n",
    "    return language_model.get_sentence_embedding(sentence)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cache size optimisation\n",
    "\n",
    "Since **LRU** (Least Recently Used) cache has BigO of $log(n)$, the smaller the better.\n",
    "But we also know that we want to cache as much as possible. So bigger the better.\n",
    "This meant we had to optimise cache maxsize empirically.\n",
    "We found 50000 as the sweet point for us.\n",
    "\n",
    "### Revised load testing method\n",
    "\n",
    "By using cache we couldn‚Äôt use just a few test samples as the cache would make them compute free.\n",
    "Hence, we had to define variable test cases so as to simulate the real text samples.\n",
    "We did this with the help of a python script to create request samples and tested with **JMeter**.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}